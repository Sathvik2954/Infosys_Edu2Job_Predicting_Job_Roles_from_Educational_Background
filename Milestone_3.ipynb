{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh5Nxl7fQhHbCDmBzu4Q8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sathvik2954/Infosys_Edu2Job_Predicting_Job_Roles_from_Educational_Background/blob/main/Milestone_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "file_name = 'processed_student_job_data.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_name)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Make sure '{file_name}' is uploaded to your Google Colab session.\")\n",
        "else:\n",
        "    print(\"Data loaded successfully.\")\n",
        "    target_column = 'Job/Higher Studies?'\n",
        "    X = df.drop(target_column, axis=1)\n",
        "    y = df[target_column]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    categorical_features = [\n",
        "        'interested career area ',\n",
        "        'Type of company want to settle in?'\n",
        "    ]\n",
        "    boolean_features = [\n",
        "        'Interested subjects_Computer Architecture', 'Interested subjects_IOT',\n",
        "        'Interested subjects_Management', 'Interested subjects_Software Engineering',\n",
        "        'Interested subjects_cloud computing', 'Interested subjects_data engineering',\n",
        "        'Interested subjects_hacking', 'Interested subjects_networks',\n",
        "        'Interested subjects_parallel computing', 'Interested subjects_programming'\n",
        "    ]\n",
        "    numeric_features = [\n",
        "        col for col in X.columns\n",
        "        if col not in categorical_features\n",
        "        and col not in boolean_features\n",
        "    ]\n",
        "\n",
        "    print(f\"Identified {len(numeric_features)} numeric features.\")\n",
        "    print(f\"Identified {len(categorical_features) + len(boolean_features)} categorical/boolean features.\")\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features + boolean_features)\n",
        "        ])\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "        \"KNN\": KNeighborsClassifier(),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        \"SVM\": SVC(random_state=42),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "        \"XGBoosting\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    }\n",
        "    results = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n--- Training {model_name} ---\")\n",
        "        clf = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', model)\n",
        "        ])\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results[model_name] = accuracy\n",
        "        print(f\"Accuracy: {accuracy:.4f} ({(accuracy * 100):.2f}%)\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"-\" * 30)\n",
        "    print(\"\\n--- Model Accuracy Summary ---\")\n",
        "    sorted_results = sorted(results.items(), key=lambda item: item[1], reverse=True)\n",
        "    for model_name, accuracy in sorted_results:\n",
        "        print(f\"{model_name}: {accuracy:.4f} ({(accuracy * 100):.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmVkbUSvfgyL",
        "outputId": "3fcb8a61-6155-4dfe-c5e0-95240d3ea305"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "Identified 33 numeric features.\n",
            "Identified 12 categorical/boolean features.\n",
            "\n",
            "--- Training Logistic Regression ---\n",
            "Accuracy: 0.5078 (50.78%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.55      0.53      2014\n",
            "           1       0.50      0.46      0.48      1986\n",
            "\n",
            "    accuracy                           0.51      4000\n",
            "   macro avg       0.51      0.51      0.51      4000\n",
            "weighted avg       0.51      0.51      0.51      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training KNN ---\n",
            "Accuracy: 0.5028 (50.28%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.50      0.50      2014\n",
            "           1       0.50      0.50      0.50      1986\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.50      0.50      0.50      4000\n",
            "weighted avg       0.50      0.50      0.50      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training Decision Tree ---\n",
            "Accuracy: 0.4973 (49.73%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50      2014\n",
            "           1       0.49      0.49      0.49      1986\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.50      0.50      0.50      4000\n",
            "weighted avg       0.50      0.50      0.50      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training SVM ---\n",
            "Accuracy: 0.5132 (51.32%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.54      0.53      2014\n",
            "           1       0.51      0.49      0.50      1986\n",
            "\n",
            "    accuracy                           0.51      4000\n",
            "   macro avg       0.51      0.51      0.51      4000\n",
            "weighted avg       0.51      0.51      0.51      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training Random Forest ---\n",
            "Accuracy: 0.4995 (49.95%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.54      0.52      2014\n",
            "           1       0.50      0.46      0.48      1986\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.50      0.50      0.50      4000\n",
            "weighted avg       0.50      0.50      0.50      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training AdaBoost ---\n",
            "Accuracy: 0.5075 (50.75%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.56      0.54      2014\n",
            "           1       0.50      0.45      0.48      1986\n",
            "\n",
            "    accuracy                           0.51      4000\n",
            "   macro avg       0.51      0.51      0.51      4000\n",
            "weighted avg       0.51      0.51      0.51      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training Gradient Boosting ---\n",
            "Accuracy: 0.5015 (50.15%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.54      0.52      2014\n",
            "           1       0.50      0.46      0.48      1986\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.50      0.50      0.50      4000\n",
            "weighted avg       0.50      0.50      0.50      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Training XGBoosting ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [16:27:43] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4960 (49.60%)\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50      2014\n",
            "           1       0.49      0.49      0.49      1986\n",
            "\n",
            "    accuracy                           0.50      4000\n",
            "   macro avg       0.50      0.50      0.50      4000\n",
            "weighted avg       0.50      0.50      0.50      4000\n",
            "\n",
            "------------------------------\n",
            "\n",
            "--- Model Accuracy Summary ---\n",
            "SVM: 0.5132 (51.32%)\n",
            "Logistic Regression: 0.5078 (50.78%)\n",
            "AdaBoost: 0.5075 (50.75%)\n",
            "KNN: 0.5028 (50.28%)\n",
            "Gradient Boosting: 0.5015 (50.15%)\n",
            "Random Forest: 0.4995 (49.95%)\n",
            "Decision Tree: 0.4973 (49.73%)\n",
            "XGBoosting: 0.4960 (49.60%)\n"
          ]
        }
      ]
    }
  ]
}